name: Daily Amazon Deals Scrape

# Run at 2 AM UTC every day, and allow manual dispatch
on:
  schedule:
    - cron: '0 2 * * *'
  workflow_dispatch:

jobs:
  fetch:
    runs-on: ubuntu-latest
    steps:

      # 1️⃣ Check out the repo (with write perms)
      - name: Check out code
        uses: actions/checkout@v3
        with:
          persist-credentials: true    # so GITHUB_TOKEN can push

      # 2️⃣ Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      # 3️⃣ Install our scraper dependencies
      - name: Install scraping libs
        run: |
          pip install requests beautifulsoup4

      # 4️⃣ Run the scraper that writes deals.json
      - name: Fetch deals via scraper
        env:
          PAAPI_PARTNER_TAG: ${{ secrets.PAAPI_PARTNER_TAG }}
        run: python fetch_deals.py

      # 5️⃣ Commit & push the updated deals.json back to main
      - name: Commit & push deals.json
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}  
        run: |
          git config user.name "github-actions"
          git config user.email "actions@github.com"
          git add deals.json
          # Only commit if there are changes
          if git diff --cached --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Daily scrape update"
            # Push using the token (note the quoted URL)
            git push "https://x-access-token:${GITHUB_TOKEN}@github.com/${{ github.repository }}.git" HEAD:main
          fi
